library(tidyverse)
library(BayesFactor)
library(shiny)
library(bslib)
library(plotly)
library(broom)
library(sass)
library(bsicons)  # for value box icons
library(shinyscreenshot)


source("plot_density_function.R")

vbs <- list(
  value_box(
    title    = HTML("Total<br/>Visitors"),
    value    = textOutput("visitorsBox"),
    showcase = bs_icon("people-fill", size = "0.6em"),
    showcase_layout = "top right",
    class    = "shadow value_gradient",
    full_screen = FALSE
  ),
  value_box(
    title    = HTML("Conversion<br/>Rate Lift"),
    value    = textOutput("upliftBox"),
    showcase = bs_icon("bar-chart-fill", size = "0.6em"),
    showcase_layout = "top right",
    class    = "shadow value_gradient",
    full_screen = FALSE
  ),
  value_box(
    title    = HTML("Control<br/>Conversion Rate"),
    value    = textOutput("ccRateBox"),
    showcase = bs_icon("activity", size = "0.6em"),
    showcase_layout = "top right",
    class    = "shadow value_gradient",
    full_screen = FALSE
  ),
  value_box(
    title    = HTML("Variation<br/>Conversion Rate"),
    value    = textOutput("varRateBox"),
    showcase = bs_icon("activity", size = "0.6em"),
    showcase_layout = "top right",
    class    = "shadow value_gradient",
    full_screen = FALSE
  ),
  value_box(
    title    = textOutput("p_value_Text"),
    value    = textOutput("pValueBox"),
    showcase = bs_icon("exclamation", size = "0.6em"),
    showcase_layout = "top right",
    class    = "shadow value_gradient",
    full_screen = FALSE
  )
)

# SET UI for First Page
first_page_ui <- page_sidebar(
  sidebar = sidebar(
    width = 530,

    tags$div(
      class = "mainContent pre-content pre-form-group",
      # tags$h1(
      #   HTML(
      #     "<span>Optimize</span> Your A/B Test Planning!"
      #   ),
      #   class = "title"
      # ),
      tags$p(
        "The pre-test calculator provides practical benefits for any business looking to conduct hypothesis testing with precision. It determines optimal sample sizes and test durations to ensure statistically significant results. Just enter the test parameters below to calculate the sample size needed and the MDE by week."
      )
    ),

    div(class = "formWrap",

      h2("Test Data", class = "form-line-title"),

      HTML('<form id="preTestForm" class="testForm">'),
      div(class = "inputGroupContainer",
        fluidRow(
          column(
            6,
            input_with_tooltip_shiny(
              input_id     = "WeeklyTraffic",
              input_type   = "numeric",
              label        = h6("Weekly Traffic", class = "form-title"),
              tooltip_text = "Expected number of total sessions or users.",
              input_args   = list(min = 100, max = 50000, value = 10000)
            )
          ),
          column(
            6,
            input_with_tooltip_shiny(
              input_id     = "Baseline",
              input_type   = "numeric",
              label        = h6("Baseline Conversion Rate (%)", class = "form-title"),
              tooltip_text = "Expected conversion rate.",
              input_args   = list(min = 1, max = 50, value = 5)
            )
          )
        ),

        # Second row for Variation A inputs
        fluidRow(
          column(
            6,
            input_with_tooltip_shiny(
              input_id     = "Conf_level",
              input_type   = "numeric",
              label        = h6("Confidence Level (%)", class = "form-title"),
              tooltip_text = "Probability that the observed difference between test variations are not due to random chance.",
              input_args   = list(min = 90, max = 99, value = 95)
            )
          ),
          column(
            6,
            input_with_tooltip_shiny(
              input_id     = "Power",
              input_type   = "numeric",
              label        = h6("Statistical Power (%)", class = "form-title"),
              tooltip_text = "The probability that the test correctly rejects the null hypothesis when the alternative hypothesis is true.",
              input_args   = list(min = 1, max = 99, value = 80)
            )
          )
        ),

        fluidRow(
          column(
            6,
            input_with_tooltip_shiny(
              input_id     = "Test_type",
              input_type   = "radio",
              label        = h6("Test Type", class = "form-title"),
              tooltip_text = "A two-sided test can detect changes in both directions, typically requiring a larger sample size or a greater lift. A one-sided test can only detect changes in one direction.",
              input_args   = list(choices = c("Two-Tailed" = "two.sided", "One-Tailed" = "one.sided"), selected = "one.sided")
            )
          )
        ),

      ),
      actionButton(
        inputId = "applyBtn1",
        label   = HTML(
          '<span class="btn-text">Calculate</span><span class="btn-arrow"></span><span class="btn-overlay""></span>'
        ),
        class   = "blue apply-btn"
      ),
      HTML("</form>"),
      br(),
      checkboxInput("showAnnotations", "Show Chart Annotations", value = TRUE),
      # actionButton("screenshot", "Take a screenshot")
    )
  ),

  page_fillable(
    card(
      plotlyOutput("plotly")
    ),
    card(
      tableOutput("table")
    )
  )

)

# SET UI for Second Page
second_page_ui <- page_sidebar(

  # theme = bslib::bs_theme() |>
  #   bs_add_rules(
  #     sass::sass_file("styles.scss") # you can also use .css
  #   ),

  sidebar = sidebar(
    width = 530,

    tags$div(
      class = "mainContent post-content post-form-group",
      # tags$h1(
      #   HTML(
      #     'Uncover the <span>Significance</span> of Your <span>A/B Test Results!</span>'
      #   ),
      #   class = "title"
      # ),
      tags$p(
        "The post-test calculator for A/B tests is an essential tool for analyzing the results of your A/B tests, providing clear, statistical evidence of whether your variation significantly outperforms the control group. It simplifies the complexity of interpreting test results while providing robust features and statistical methods. Just enter the test parameters below to calculate statistical significance, confidence intervals for each variation, the p-value and to understand if sample ration mismatch (SRM) is affecting your test results."
      )
    ),

    div(class = "formWrap",

      h2("Test Data", class = "form-line-title"),
      HTML('<form id="postTestForm" class="testForm">'),
      div(class = "inputGroupContainer",
        fluidRow(
          column(
            6,
            input_with_tooltip_shiny(
              input_id = "controlSampSize",
              input_type = "numeric",
              label = h6("Control Sample Size", class = "form-title"),
              tooltip_text = "Number of users in the control group during the experiment.",
              input_args = list(value = 10000)
            )
          ),
          column(
            6,
            input_with_tooltip_shiny(
              input_id = "controlConversions",
              input_type = "numeric",
              label = h6("Control Conversions", class = "form-title"),
              tooltip_text = "Number of conversions in the control group.",
              input_args = list(value = 1110)
            )
          )
        ),

        # Second row for Variation A inputs
        fluidRow(
          column(
            6,
            input_with_tooltip_shiny(
              input_id = "varSampSize",
              input_type = "numeric",
              label = h6("Variation Sample Size", class = "form-title"),
              tooltip_text = "Number of users in the variation group during the experiment.",
              input_args = list(value = 10000)
            )
          ),
          column(
            6,
            input_with_tooltip_shiny(
              input_id = "varConversions",
              input_type = "numeric",
              label = h6("Variation Conversions", class = "form-title"),
              tooltip_text = "Number of conversions in the variation group.",
              input_args = list(value = 1200)
            )
          )
        ),
      ),
      tags$div(
        class = "post-settings",

        h2("Settings", class = " form-line-title"),
        # Removed tooltip from this drop down
        selectInput(
          inputId = "stat_test_type",
          label = h6("Method", class = "form-title"),
          choices = c("Frequentist" = "Frequentist"),
          selected = "Frequentist"
        ),
        fluidRow(
          column(
            6,
            input_with_tooltip_shiny(
              input_id = "confidenceLevel",
              input_type = "radio",
              label = h6("Confidence Level", class = "form-title"),
              tooltip_text = "Probability that the observed difference between test variations are not due to random chance.",
              input_args = list(
                choices = c(
                  "90%" = 0.90,
                  "95%" = 0.95,
                  "99%" = 0.99
                ),
                selected = 0.95
              )
            )
          ),

          # Confidence Level
          column(
            6,
            conditionalPanel(
              condition = "input.stat_test_type == 'Frequentist'",
              input_with_tooltip_shiny(
                input_id = "testType",
                input_type = "radio",
                label = h6("Hypothesis", class = "form-title"),
                tooltip_text = "A two-sided test can detect changes in both directions, typically requiring a larger sample size or a greater lift. A one-sided test can only detect changes in one direction.",
                input_args = list(
                  choices = c("One-sided", "Two-sided"),
                  selected = "One-sided"
                )
              )
            )

          )
        )
      ),
      actionButton(
        inputId = "applyBtn2",
        label   = HTML(
          '<span class="btn-text">Calculate</span><span class="btn-arrow"></span><span class="btn-overlay""></span>'
        ),
        class   = "blue apply-btn"
      ),
      HTML("</form>"),
    ),
    br(),
    # actionButton("screenshot2", "Take a screenshot"),
    # card_footer(
    #     max_height = 100,
    #     HTML("<strong style='font-size: 1.1em; color: #707070;'>Supported Test Types</strong>"),
    #     HTML("<strong style='font-size: 0.9em; color: #707070;'><br/>One-Tailed (Default):</strong><br/><span style='font-size: 0.85em; color: #707070;'>Used to determine if a treatment significantly outperforms the control, typically to identify which is better.</span><br/>"),
    #     HTML("<strong style='font-size: 0.9em; color: #707070;'>Two-Tailed:</strong><br/><span style='font-size: 0.85em; color: #707070;'>Assesses whether the variant is either superior or inferior to the control, focusing on identifying any significant differences.</span>")
    #   )
  ),

  page_fillable(
    fluidRow(
      # First column with the specified width for resultBox and conversionComparisonPlot
      layout_column_wrap(
        class = "value_boxes",
        fill  = FALSE,
        # height_mobile = 450,
        !!!vbs
      ),
      layout_columns(
        class = "layout_boxes",
        col_widths = c(4, 3, 5),

        card(
          height = 200,
          min_height = 100,
          max_height = 200,
          card_header(
            "Test Results",
          ),
          uiOutput("resultBox"), class = "shadow"
        ),
        card(
          height = 200,
          min_height = 100,
          max_height = 200,
          card_header(
            "Sample Ratio Mismatch",
          ),
          uiOutput("srmBox")
        ),
        card(
          height = 200,
          min_height = 100,
          max_height = 200,
          card_header(
            "Confidence Interval",
          ),
          plotlyOutput("conversionComparisonPlot"), class = "shadow"
        )
      ),
      # Second column with the specified width for value_boxes

    ),
    card(
      card_header(
        "Conversion Rate Distribution of Control and Variation",
      ),
      plotlyOutput("densityComparisonPlot"),
      min_height = 250
    )
  )

)


# Define the overall UI with page_navbar
ui <- page_navbar(

  tags$script(src = "formWarning.js"),
  tags$script(src = "greyOut.js"),

  theme = bslib::bs_theme() |>
    bs_add_rules(
      sass::sass_file("styles.scss") # you can also use .css
    ),
  title = "AB Testing Calculators",
  tabPanel(title = "Pre-test", first_page_ui),
  tabPanel(title = "Post-test", second_page_ui)

)

## SERVER

shinyApp(ui, function(input, output) {

  ## PRE-TEST EVALUATION ##

  # Validation Logic for pre-test
  validation_checks_pre <- reactive({
    validate(
      need(input$WeeklyTraffic >= 190 && input$WeeklyTraffic <= 100000000, "Weekly Traffic must be between 190 and 100,000,000"),
      need(input$Baseline      > 0 && input$Baseline <= 92, "Baseline conversion rate must be greater than 0 and less than 93%"),
      need(input$Conf_level      >= 90 && input$Conf_level <= 99, "Confidence level must be between 90 and 99"),
      need(input$Power         >= 1 && input$Power <= 99, "Statistical power must be between 1 and 99")
    )
  })

  # observeEvent(input$screenshot, {
  #   screenshot(filename = "ab-pre-testing")
  # })
  #
  # observeEvent(input$screenshot2, {
  #   screenshot(filename = "ab-post-testing")
  # })

  ## PRE-TEST CALCULATOR ##

  # data_combined <- eventReactive(input$applyBtn1, {
  #
  #   validation_checks_pre()
  #
  #   traffic_seq <- seq(input$WeeklyTraffic, input$WeeklyTraffic * 8, by = input$WeeklyTraffic)
  #   map_df(traffic_seq, ~ power.prop.test(
  #     p1          = input$Baseline / 100,
  #     p2          = NULL,
  #     n           = .x / 2,
  #     power       = input$Power / 100,
  #     sig.level   = input$SigLevel / 100,
  #     alternative = input$Test_type,
  #     strict      = TRUE
  #   ) %>%
  #     tidy() %>%
  #     mutate(effect_raw     = p2 / p1 - 1,
  #            total          = n,
  #            n              = n,
  #            weeks          = row_number(),
  #            effect_percent = scales::percent(effect_raw, accuracy = 0.01, big.mark = ","),
  #            n_comma        = scales::comma(n),
  #            Weeks          = paste0(row_number(), " Weeks")
  #            # ,
  #            # difference     = abs(effect_raw - input$MDE / 100)
  #     )
  #   )
  # }, ignoreNULL = FALSE)


  ## NEW MDE CALCULATION
  data_combined <- eventReactive(input$applyBtn1, {
    validation_checks_pre()

    traffic_seq <- seq(input$WeeklyTraffic, input$WeeklyTraffic * 8, by = input$WeeklyTraffic)
    map_df(traffic_seq, ~ {
      n            <- .x / 2
      p1           <- input$Baseline / 100
      significance <- 1 - (input$Conf_level / 100)
      power        <- input$Power / 100
      test_type    <- input$Test_type

      # Two-tailed or one-tailed test
      if (test_type == "two.sided") {
        a <- qnorm(1 - significance / 2)
      } else {
        a <- qnorm(1 - significance)
      }
      b <- qnorm(power)
      c <- n / (a + b)^2

      p2 <- (2 * p1 * c + 1 + sqrt(-8 * p1^2 * c - 4 * p1^2 + 8 * p1 * c + 4 * p1 + 1)) / (2 * (c + 1))
      effect_raw <- (p2 - p1) / p1

      week_number <- .x / input$WeeklyTraffic

      # Creating additional columns
      tibble(
        effect_raw     = effect_raw,
        total          = n * 2,
        n              = n,
        weeks          = week_number,
        effect_percent = scales::percent(effect_raw, accuracy = 0.01, big.mark = ","),
        n_comma        = scales::comma(n),
        Weeks          = paste0(week_number, " Weeks")
      )
    })
  }, ignoreNULL = FALSE)

  ## FOR ANNOTATIONS TOGGLE
  output$plotly <- renderPlotly({

    plot_data <- data_combined() |>
      mutate(weeks = row_number())

    # Create the basic plot
    fig <- plot_ly(
      plot_data,
      x = ~n,
      y = ~effect_raw,
      type = "bar",
      hoverinfo = "text",
      marker = list(color = "rgba(191, 53, 142, 0.35)")
    )

    # Customizing the layout
    fig <- fig |> layout(
      showlegend = FALSE,
      yaxis = list(title = "MDE", tickformat = ",.0%", showgrid = FALSE),
      xaxis = list(title = "Sample Size Per Group", showgrid = FALSE),
      font = list(family = "Open Sans, Roboto, Arial, sans-serif", size = 12, color = "#000000")
    ) |> config(displayModeBar = FALSE)

    # Conditionally add annotations if checkbox is checked
    if (input$showAnnotations) {
      fig <- fig %>%
        add_annotations(
          x = plot_data$n,
          y = plot_data$effect_raw,
          text = ~paste("MDE: ", scales::percent(effect_raw, big.mark = ","), "<br>Per Group: ", scales::comma(n)),
          showarrow = FALSE,
          xanchor = "center",
          align = "center",
          font = list(family = "Open Sans, Roboto, Arial, sans-serif", size = 11, color = "#601B47")
        )
    }

    fig
  })


  ## Table render
  output$table <- renderTable({

    data_combined() |>
      mutate(`# of Weeks` = row_number()) |>
      select(
        `# of Weeks`,
        `Relative Minimal Detectible Effect (MDE)` = effect_percent,
        `Sample Size Per Variant`                    = n_comma
      )
  },
  selection = "single", class = "display nowrap compact",
  filter = "none", rownames = FALSE, hover = TRUE, spacing = "m",
  align = "c", width = "100%", height = "auto"
  )

  ## POST-TEST EVALUATION ##

  # Validation Logic for post-test
  validation_checks <- reactive({
    validate(
      need(input$controlSampSize >= 0, "Control group must be non-negative"),
      need(input$varSampSize >= 0, "Variation A must be non-negative"),
      need(input$controlConversions > 0, "Control conversions must be non-negative"),
      need(input$varConversions > 0, "Variation conversions must be non-negative"),
      need(input$controlSampSize >= input$controlConversions, "Control conversions must not exceed control sample"),
      need(input$varSampSize >= input$varConversions, "Variation conversions must not exceed variation sample")
    )
  })

  ## POST TEST EVALUATION ##
  all_reactive_data <- eventReactive(input$applyBtn2, {

    validation_checks()


    visitors    <- c(A = input$controlSampSize,    B = input$varSampSize)
    conversions <- c(A = input$controlConversions, B = input$varConversions)
    # SRM integration
    total_visitors <- sum(visitors)
    # Expected Proportion
    expected_proportion  <- 0.5
    # Chi-Square Test proportions
    expected_proportions <- rep(expected_proportion, 2)
    # Chi-Square Test
    chi_square_test_p <- chisq.test(visitors, p = expected_proportions)$p.value
    # Determine mismatch outcome
    srm_mismatch <- ifelse(any(chi_square_test_p > 0.001), 1, 0)


    if (any(visitors < 0, conversions < 0, conversions > visitors, is.na(c(visitors, conversions)))) {
      return(NULL)  # Return NULL if inputs are not valid
    }

    # Compute rates
    rates <- conversions / visitors

    # Compute confidence metrics
    conf_level    <- as.numeric(input$confidenceLevel)
    z_score       <- qnorm((1 + conf_level) / 2)
    alpha_value   <- 1 - conf_level

    # p-value calculation
    test_type <- input$testType

    # Simulations for density plot
    prior <- c(1, 1)
    post  <- rbind(
      c(conversions["A"] + prior[1], visitors["A"] - conversions["A"] + prior[2]),
      c(conversions["B"] + prior[1], visitors["B"] - conversions["B"] + prior[2])
    )
    set.seed(123)
    simulations <- list(A = rbeta(100000, post[1, 1], post[1, 2]),
                        B = rbeta(100000, post[2, 1], post[2, 2]))

    dens_A <- density(simulations$A, n = 2048, adjust = 2)
    dens_B <- density(simulations$B, n = 2048, adjust = 2)

    # Add conversion rate calculations
    conversion_rate_A <- rates["A"]
    conversion_rate_B <- rates["B"]


    # Additional computations
    prob_B_better_than_A            <- mean(simulations$B > simulations$A)
    overall_rate                    <- sum(conversions) / sum(visitors)
    standard_errors                 <- sqrt(rates * (1 - rates) / visitors)
    relative_uplift_conversion_rate <- (rates["B"] - rates["A"]) / rates["A"]
    z_score_for_p                   <- (rates["B"] - rates["A"]) / sqrt(sum(standard_errors ^ 2))

    # Create tibble for Bayes probability bar chart
    simulation_result_tbl <- tibble(
      group = c("Control", "Variation"),
      prob  = c(mean(simulations$A > simulations$B), mean(simulations$B > simulations$A))
    )


    # Determine the p-value based on the selected test type
    p_value <- if (test_type == "Two-sided") {
      2 * pnorm(-abs(z_score_for_p))
    } else if (test_type == "One-sided") {
      pnorm(z_score_for_p, lower.tail = FALSE)
    } else {
      NULL  # In case of an unexpected test type
    }

    # Comparison Data Frame
    comparison_df <- tibble(
      type       = names(rates),
      rate       = rates,
      std_err    = standard_errors,
      conf_lower = rates - z_score * standard_errors,
      conf_upper = rates + z_score * standard_errors
    )

    # Output of all_reactive_data
    list(
      dens_A                          = dens_A,
      dens_B                          = dens_B,
      conversion_rate_A               = conversion_rate_A,
      conversion_rate_B               = conversion_rate_B,
      total_visitors                  = total_visitors,
      simulation_result_tbl           = simulation_result_tbl,
      z_score_for_p                   = z_score_for_p,
      p_value                         = p_value,
      srm_mismatch                    = srm_mismatch,
      rates                           = rates,
      overall_rate                    = overall_rate,
      standard_errors                 = standard_errors,
      z_score                         = z_score,
      alpha_value                     = alpha_value,
      simulations                     = simulations,
      prob_B_better_than_A            = prob_B_better_than_A,
      comparison_df                   = comparison_df,
      relative_uplift_conversion_rate = relative_uplift_conversion_rate
    )
  }, ignoreNULL = FALSE)

  # ## PLOTS

  output$conversionComparisonPlot <- renderPlotly({
    ## FREQUENTIST PLOT
    if (input$stat_test_type == "Frequentist") {
      comparison_df <- all_reactive_data()$comparison_df

      # Ensure 'type' is a factor and handle NA values
      comparison_df$type <-
        factor(comparison_df$type, levels = unique(comparison_df$type))
      comparison_df <-
        na.omit(comparison_df)  # Remove rows with NA values

      comparison_df <-
        comparison_df |>
        mutate(type = case_when(type == "A" ~ "Control",
                                type == "B" ~ "Variation"))

      plot_ly(
        comparison_df,
        y = ~ type,
        x = ~ rate,
        type = "scatter",
        mode = "markers",
        error_x = ~ list(array = rate - conf_lower, arrayminus = conf_upper - rate),
        color = ~ type,
        text = ~ paste(scales::percent(rate)),
        hoverinfo = "text",
        # Assign colors based on the 'type'
        colors = c(
          "Control" = "#2D455F",
          "Variation" = "#BF358E"
        )
      ) |>
        layout(
          showlegend = FALSE,
          yaxis = list(
            title = "",
            showgrid = FALSE,
            automargin = TRUE,
            tickvals   = ~ type,
            ticktext   = ~ paste(type, "   "),
            range = c(-0.4, 1.15)
          ),
          xaxis = list(
            title = "",
            tickformat = ",.0%",
            showgrid = FALSE,
            titlefont = list(size = 12)
          )
        ) |>
        layout(
          showlegend = FALSE,
          yaxis = list(title = "", showgrid = FALSE),
          xaxis = list(
            title = "",
            tickformat = ",.0%",
            showgrid = FALSE
          )
        ) |>
        add_annotations(
          x = comparison_df$rate,
          y = comparison_df$type,
          text = ~ paste(sprintf("%.1f", rate * 100), "%</b>"),
          showarrow = FALSE,
          xanchor = "center",
          yanchor = "bottom",
          yshift = 10,
          font = list(size = 10)
        ) |>
        add_annotations(
          x = comparison_df$conf_lower,
          y = comparison_df$type,
          text = ~ paste(sprintf("%.1f", conf_lower * 100), "%"),
          showarrow = FALSE,
          xanchor = "center",
          yanchor = "bottom",
          yshift = 10,
          font = list(size = 10)
        ) |>
        add_annotations(
          x = comparison_df$conf_upper,
          y = comparison_df$type,
          text = ~ paste(sprintf("%.1f", conf_upper * 100), "%"),
          showarrow = FALSE,
          xanchor = "center",
          yanchor = "bottom",
          yshift = 10,
          font = list(size = 10)
        ) |>
        config(displayModeBar = FALSE, editSelection = FALSE)

      ## Bayesian plot
    } else {
      simulation_result_tbl <- all_reactive_data()$simulation_result_tbl
      plot_ly(
        simulation_result_tbl,
        y = ~ group,
        x = ~ prob,
        # Using variables from your ggplot code
        type = "bar",
        orientation = "h",
        text = ~ paste(scales::percent(prob)),
        hoverinfo = "text",
        marker = list(color = c("#2D455F", "#BF358E"))
      ) %>%
        layout(
          yaxis = list(
            title      = "",
            showgrid   = FALSE,
            automargin = TRUE,
            tickvals   = ~ group,
            ticktext   = ~ paste(group, "   ")  # Add spaces before the group label)
          ),
          xaxis = list(
            title = "% Chance of outperforming",
            tickformat = ",.0%",
            range = c(0, 1.05),
            showgrid = FALSE,
            titlefont = list(size = 12)
          )
        ) %>%
        config(displayModeBar = FALSE)
    }
  })



  # Density plot
  output$densityComparisonPlot <- renderPlotly({

    ## For plotly plot

    data <- all_reactive_data()

    if (is.null(data$simulations) || is.null(data$rates)) {
      return()
    }

    # Extract the densities and conversion rates
    dens_A            <- data$dens_A
    dens_B            <- data$dens_B
    conversion_rate_A <- data$conversion_rate_A
    conversion_rate_B <- data$conversion_rate_B

    # Generate the Plotly plot
    p <- plot_ly() %>%
      add_trace(x = dens_A$x, y = dens_A$y, type = "scatter", mode = "lines",
                line = list(color = "rgba(45, 69, 95, 1)", width = 1), name = "Control",
                fill = "tozeroy", fillcolor = "rgba(45, 69, 95, 0.25)") %>%
      add_trace(x = dens_B$x, y = dens_B$y, type = "scatter", mode = "lines",
                line = list(color = "rgba(191, 53, 142, 1)", width = 1), name = "Variation",
                fill = "tozeroy", fillcolor = "rgba(191, 53, 142, 0.25)") %>%
      add_lines(x = c(conversion_rate_A, conversion_rate_A), y = c(0, max(dens_A$y, dens_B$y)),
                line = list(color = "rgba(45, 69, 95, 0.7)", dash = "dash", width = 1), name = "Control") %>%
      add_lines(x = c(conversion_rate_B, conversion_rate_B), y = c(0, max(dens_A$y, dens_B$y)),
                line = list(color = "rgba(191, 53, 142, 0.7)", dash = "dash", width = 1), name = "Variation") %>%
      add_annotations(x = conversion_rate_A, y = max(dens_A$y, dens_B$y) * 1.04, text = sprintf("%.2f%%", conversion_rate_A * 100),
                      xref = "x", yref = "y", showarrow = FALSE, font = list(size = 12)) %>%
      add_annotations(x = conversion_rate_B, y = max(dens_A$y, dens_B$y) * 1.04, text = sprintf("%.2f%%", conversion_rate_B * 100),
                      xref = "x", yref = "y", showarrow = FALSE, font = list(size = 12)) %>%
      layout(
        xaxis = list(title = "Conversion Rate", tickformat = ".2%"),
        yaxis = list(showgrid = FALSE, showticklabels = FALSE),
        showlegend = FALSE
      ) %>%
      config(displayModeBar = FALSE)

    # Print the plot
    p

  })


  ## Values and Output boxes

  # Box visitors
  output$visitorsBox <- renderText({
    visitors <- all_reactive_data()$total_visitors
    if (is.null(visitors)) {
      "N/A"
    } else {
      scales::comma(visitors)
    }
  })

  # Box uplift
  output$upliftBox <- renderText({
    uplift <- all_reactive_data()$relative_uplift_conversion_rate
    if (is.null(uplift)) {
      "N/A"
    } else {
      scales::percent(uplift, accuracy = 0.01)
    }
  })


  output$ccRateBox <- renderText({
    rates <- all_reactive_data()$rates
    rateA <- rates["A"]

    if (is.null(rateA)) {
      "N/A"
    } else {
      scales::percent(rateA, accuracy = 0.01)
    }
  })


  output$varRateBox <- renderText({
    rates <- all_reactive_data()$rates
    rateB <- rates["B"]

    if (is.null(rateB)) {
      "N/A"
    } else {
      scales::percent(rateB, accuracy = 0.01)
    }
  })


  # Box z-score
  output$zScoreBox <- renderText({
    z_score <- all_reactive_data()$z_score_for_p
    if (is.null(z_score)) {
      "N/A"
    } else {
      round(z_score, 4)
    }
  })


  # Output of p-value
  output$pValueBox <- renderText({
    p_value <- all_reactive_data()$p_value

    # Display the p-value in a value box
    if (is.null(p_value)) {
      "N/A"
    } else {
      round(p_value, 4)
    }
  })


  output$pValueBox <- renderText({

    if (input$stat_test_type == "Frequentist") {
      p_value <- all_reactive_data()$p_value

      if (p_value < 0.0001) {
        p_value_formatted <- "< 0.0001"
      } else if (p_value > 0.9999) {
        p_value_formatted <- "> 0.9999"
      } else {
        p_value_formatted <- formatC(p_value, format = "f", digits = 4)
      }
    } else {
      p_value <- all_reactive_data()$prob_B_better_than_A * 100

      if (p_value > 99.99) {
        p_value_formatted <- "> 99.99%"
      } else if (p_value < 0.01) {
        p_value_formatted <- "< 0.01%"
      } else {
        p_value_formatted <- paste0(round(p_value, 2), "%")
      }
    }

    # Display the p-value in a value box
    if (is.null(p_value)) {
      "N/A"
    } else {
      p_value_formatted
    }
  })


  ## Change text of title
  output$p_value_Text <- renderText({

    if (input$stat_test_type == "Frequentist") {
      title_text <- "P-Value"
    } else {
      title_text <- "Probability Variation is Better"
    }
  })


  ## SRM Output box
  output$srmBox <- renderUI({
    srm_mismatch <- all_reactive_data()$srm_mismatch
    if (srm_mismatch == 1) {
      tagList(
        # h5("Sample Ratio Mismatch (SRM) Detection", class="title"),
        div(class = "passing-container pcr",
          HTML("
            <div class='boxWrapper boxGrid'>
              <div class='radar'>
                <svg xmlns='http://www.w3.org/2000/svg' width='16' height='16' viewBox='0 0 16 16'>
                <path d='M6.0001 10.78L3.2201 7.99999L2.27344 8.93999L6.0001 12.6667L14.0001 4.66665L13.0601 3.72665L6.0001 10.78Z'/>
                </svg>
              </div>
              <div class='message'>
                No SRM Detected.
              </div>
            </div>
          ")
        )
      )
    } else {
      tagList(
        # h5("Sample Ratio Mismatch (SRM) Detection",class="title"),
        div(class = "warning-container",
          HTML("
            <div class='boxWrapper boxGrid'>
              <div class='radar'>
                <svg xmlns='http://www.w3.org/2000/svg' height='16' width='2' viewBox='0 0 64 512'>
                  <path d='M64 64c0-17.7-14.3-32-32-32S0 46.3 0 64V320c0 17.7 14.3 32 32 32s32-14.3 32-32V64zM32 480a40 40 0 1 0 0-80 40 40 0 1 0 0 80z'/>
                </svg>
              </div>
              <div class='message'>
                <span class='red-warning'>A SRM Has Been Detected.</span>
              </div>
            </div>
            <div class='desc'>
              While it warrants a review of test execution and user allocation, a minor SRM often doesn't compromise the overall validity of the test results.
            </div>
          ")
        )
      )
    }
  })


  result <- eventReactive(input$applyBtn2, {
    ### FREQUENTIST SELECTED ###
    validation_checks()

    if (input$stat_test_type == "Frequentist") {

      validation_checks()

      alpha_value     <- all_reactive_data()$alpha_value
      p_value         <- all_reactive_data()$p_value
      relative_uplift <- all_reactive_data()$relative_uplift_conversion_rate
      test_type       <- input$testType  # Assuming 'testType' is the input ID for the test type selection

      # Ensure that all necessary values are available
      if (!is.null(alpha_value) && !is.null(p_value)) {
        if (test_type == "Two-sided") {
          # Two-sided test specific logic
          if (p_value < alpha_value) {
            tagList(
              # h5("Test Result", class="title"),
              div(class="passing-container",
                HTML("
                  <div class='boxWrapper'>
                    <div class='message'>
                      The Test Is Significant!
                    </div>
                  </div>
                  <div class='desc'>
                    <p>There is evidence of a difference in performance between the control and the variation.</p>
                  </div>
                ")
              )
            )
          } else {
            tagList(
              # h5("Test Result", class="title"),
              div(class = "warning-container",
                HTML("
                  <div class='boxWrapper'>
                    <div class='message'>
                      <span class='red-warning'>
                        The Test Is Not Significant!
                      </span>
                    </div>
                  </div>
                  <div class='desc'>
                    <p>The difference is not large enough to declare a significant winner.</p>
                    <p>There is no real difference in performance between the control and the variation or you need to collect more data.</p>
                  </div>
                ")
              )
            )
          }
        } else {
          # Original logic for non-two-sided tests
          if (!is.null(relative_uplift)) {
            if (p_value < alpha_value) {
              if (relative_uplift < 0) {
                # If p-value is less than alpha and relative uplift is negative
                tagList(
                  # h5("Test Result", class="title"),
                  div(class = "passing-container",
                    HTML("
                      <div class='boxWrapper'>
                        <div class='message'>
                          The Test Is Significant!
                        </div>
                      </div>
                      <div class='desc'>
                        <p>There is evidence of a difference in performance between the control and variation, <span class='negative'>however, the performance of the variation is worse than the control group.</span></p>
                      </div>
                    ")
                  )
                )
              } else {
                # If p-value is less than alpha and relative uplift is not negative
                tagList(
                  # h5("Test Result", class="title"),
                  div(class = "passing-container",
                    HTML("
                      <div class='boxWrapper'>
                        <div class='message'>
                          The Test Is Significant!
                        </div>
                      </div>
                      <div class='desc'>
                        <p>The difference is large enough to declare a significant winner.</p>
                        <p>There is evidence that the performance of the variation is better than the control.</p>
                      </div>
                    ")
                  )
                )
              }
            } else {
              # If p-value is not less than alpha
              tagList(
                # h5("Test Result", class="title"),
                div(class = "warning-container",
                  HTML("
                    <div class='boxWrapper'>
                      <div class='message'>
                        <span class='red-warning'>
                          The Test Is Not Significant!
                        </span>
                      </div>
                    </div>
                    <div class='desc'>
                      <p>The difference is not large enough to declare a significant winner.</p>
                      <p>There is no evidence that your variation is better than the control.</p>
                    </div>
                  ")
                )
              )
            }
          } else {
            # Default display when values are not available
            tagList(
              # h5("Test Result", class="title"),
              div(class="warning-container",
                HTML("
                  <div class='boxWrapper'>
                    <div class='message'>
                      <span class='red-warning'>
                        Insufficient data to determine test result.
                      </span>
                    </div>
                  </div>
                ")
              )
            )
          }
        }
      } else {
        # Default display when values are not available
        tagList(
          # h5("Test Result", class="title"),
          div(class = "warning-container",
            HTML("
              <div class='boxWrapper'>
                <div class='message'>
                  <span class='red-warning'>
                    Insufficient data to determine test result.
                  </span>
                </div>
              </div>
            ")
          )
        )
      }
    } else {

      ### BAYESIAN SELECTED ###

      conf_level           <- input$confidenceLevel
      prob_B_better_than_A <- all_reactive_data()$prob_B_better_than_A

      if (!is.null(prob_B_better_than_A) && prob_B_better_than_A > conf_level) {
        tagList(
          # h5("Test Result", class="title"),
          div(class = "passing-container",
            HTML("
              <div class='boxWrapper'>
                <div class='message'>
                  You Have A Winner!
                </div>
              </div>
              <div class='desc'>
                <p>The probability that the variation is better than the control is greater than your confidence level. You can be confident the variation is better than the control.</p>
              </div>
            ")
          )
        )
      } else {
        tagList(
          # h5("Test Result", class="title"),
          div(class = "warning-container",
            HTML("
              <div class='boxWrapper'>
                <div class='message'>
                  <span class='red-warning'>
                    You Don't Have A Winner!
                  </span>
                </div>
              </div>
              <div class='desc'>
                <p>The probability that the variation is better than the control is not greater than your confidence level. There is not enough evidence to suggest the performance of the variation is better than the control or you need to collect more data.</p>
              </div>
            ")
          )
        )
      }
    }
  }, ignoreNULL = FALSE)


  # Update the resultBox output
  output$resultBox <- renderUI({
    result()
  })

  commonContent <- reactive({
    srm_mismatch <- all_reactive_data()$srm_mismatch
    if (srm_mismatch == 1) {
      tagList(
        div(class = "passing-container pcr",
          HTML("
            <div class='boxWrapper boxGrid'>
              <div class='radar'>
                <svg xmlns='http://www.w3.org/2000/svg' width='16' height='16' viewBox='0 0 16 16'>
                  <path d='M6.0001 10.78L3.2201 7.99999L2.27344 8.93999L6.0001 12.6667L14.0001 4.66665L13.0601 3.72665L6.0001 10.78Z'/>
                </svg>
              </div>
              <div class='message'>
                No SRM Detected.
              </div>
            </div>
          ")
        )
      )
    } else {
      tagList(
        div(class = "warning-container",
          HTML("
            <div class='boxWrapper boxGrid'>
              <div class='radar'>
                <svg xmlns='http://www.w3.org/2000/svg' height='16' width='2' viewBox='0 0 64 512'>
                  <path d='M64 64c0-17.7-14.3-32-32-32S0 46.3 0 64V320c0 17.7 14.3 32 32 32s32-14.3 32-32V64zM32 480a40 40 0 1 0 0-80 40 40 0 1 0 0 80z'/>
                </svg>
              </div>
              <div class='message'>
                <span class='red-warning'>A SRM Has Been Detected.</span>
              </div>
            </div>
            <div class='desc'>
              While it warrants a review of test execution and user allocation, a minor SRM often doesn't compromise the overall validity of the test results.
            </div>
          ")
        )
      )
    }
  })

  # SRM Output box
  # output$srmBox <- renderUI({
  #   commonContent()
  # })

  # Update the resultBox output
  output$resultBox <- renderUI({
    tagList(
      result()
      # ,
      # commonContent()
    )
  })

  # Combine both renderings in the same div container
  output$combinedBoxes <- renderUI({
    tagList(
      output$resultBox
    )
  })

})
